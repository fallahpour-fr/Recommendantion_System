{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import copy\n",
    "# from utils import *\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer():\n",
    "    \"\"\"\n",
    "    A linear layer module which calculate (Wx + b).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim_in, dim_out, initializer, reg, alpha):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            - dim_in: input dimension,\n",
    "            - dim_out: output dimension,\n",
    "            - initializer: a function which get (dim_in, dim_out) and initialize\n",
    "                a [dim_in x dim_out] matrix,\n",
    "            - reg: L2-regularization flag\n",
    "            - alpha: L2-regularization coefficient\n",
    "        \"\"\"\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dim_out\n",
    "        self.params = {\n",
    "            #########################################\n",
    "            ##          Initialize parameters      ##\n",
    "            ##              Your Code              ##\n",
    "            #########################################\n",
    "            'W': None,\n",
    "            'b': None\n",
    "        }\n",
    "        self.grads = dict()\n",
    "        self.cache = dict()\n",
    "\n",
    "    def _forward(self, x):\n",
    "        \"\"\"\n",
    "        linear forward function, calculate Wx+b for a batch of data\n",
    "\n",
    "        Args:\n",
    "            x : a batch of data\n",
    "         Note:\n",
    "            you need to store some values in cache to be able to\n",
    "            calculate backward path.\n",
    "        \"\"\"\n",
    "        #########################################\n",
    "        ##              Your Code              ##\n",
    "        #########################################\n",
    "        y = None\n",
    "\n",
    "        return y\n",
    "\n",
    "    def backward(self, upstream):\n",
    "        \"\"\"\n",
    "        get upstream gradient and returns downstream gradient\n",
    "\n",
    "        Args:\n",
    "            upstream : upstream gradient of loss w.r.t module output\n",
    "\n",
    "        Note:\n",
    "            you need to calculate gradient of loss w.r.t module input\n",
    "            and parameters and store them in grads.\n",
    "        \"\"\"\n",
    "        #########################################\n",
    "        ##              Your Code              ##\n",
    "        #########################################\n",
    "        grad_b = None\n",
    "        grad_w = None\n",
    "        grad_x = None\n",
    "        grad_reg = None\n",
    "\n",
    "        self.grads = {\n",
    "            'W': grad_w,\n",
    "            'b': grad_b,\n",
    "            'x': grad_x,\n",
    "            'reg': grad_reg\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check, output must be from o(e-5)\n",
    "initializer = lambda x, y: np.random.normal(size=(y, x))\n",
    "linear = LinearLayer(5, 10, initializer, reg=True, alpha=0.001)\n",
    "check_gradient_linear(linear, h=0.00001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
